# Chatbot PLN con Ollama + RAG

Un bot en Telegram que utiliza un modelo local de Ollama (‚Äúllama3.2:3b‚Äù) junto con RAG para responder preguntas sobre productos METRO (Per√∫).

---

## üìã Prerrequisitos

- **Python 3.10+**  
- **Ollama** instalado y modelo `llama3` disponible (`ollama run llama3`)  
- **ngrok** (o t√∫nel equivalente)  
- Token de Telegram (`TELEGRAM_TOKEN`) obtenido de [@BotFather](https://t.me/BotFather)  
- Clave de acceso privada (`CHATBOT_KEY`) para `/auth`

---

## üîß Instalaci√≥n

```bash
# 1. Clona el repositorio
git clone https://github.com/tu-org/chatbot-pln.git
cd chatbot-pln

# 2. Crea y activa el entorno virtual
python3 -m venv venv
source venv/bin/activate    # macOS/Linux
# .\venv\Scripts\activate   # Windows PowerShell

# 3. Instala dependencias
pip install -r requirements.txt
````

---

## ‚öôÔ∏è Configuraci√≥n

En la ra√≠z del proyecto, crea un archivo `.env` con:

```ini
TELEGRAM_TOKEN=123456:ABC-DEF‚Ä¶
BOT_PASSWORD=tuClaveSecreta
PORT=8443
# Opcional: si no usas auto-descubrimiento de ngrok,
# copia aqu√≠ la URL HTTPS que te da ngrok
NGROK_URL=https://abcd1234.ngrok-free.app
```

Y aseg√∫rate, en otra terminal, de tener Ollama corriendo:

```bash
ollama run llama3.2:3b
```

---

## üöÄ Ejecuci√≥n local (Webhook + ngrok)

1. En un terminal, inicia ngrok apuntando al puerto de tu bot:

   ```bash
   ngrok http 8443
   ```
2. En otro terminal, lanza el script de arranque:

   ```bash
   python start.py
   # o bien
   ./start.sh
   ```

   El script:

   * Carga las variables de `.env`
   * Arranca ngrok (si no lo has iniciado manualmente)
   * Espera la URL HTTPS y la exporta a `NGROK_URL`
   * Ejecuta tu bot (`main.py`) en modo webhook

---

## üìù Uso

1. En Telegram, env√≠a `/start`.
2. Autent√≠cate con `/auth <tu-clave>`.
3. ¬°Ya puedes conversar con el bot!

